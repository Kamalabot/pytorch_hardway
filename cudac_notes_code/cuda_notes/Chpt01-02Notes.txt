Chapters 01 and 02 Notes:

- Task & Data Parallelism concept, where CUDA is using the Data Parallelism.
- Next is partitioning Data, where Block / cyclic partitioning is avbl  
    - Block takes 1 portion of Data while cyclic takes more than one portion of the data
- Single Instruction Multiple Data refers to Parallel architecture.

- Some terms:
- Latency: Time it takes for operation to start and complete, referred in MicroSeconds.
- BandWidth: Amount of Data that can be processed in unit of time. 
- Throughput: Amount of operation that can processed per unit time, refered as floating Point Ops per sec.
    - These three are limiting resources, which are required to be optimized with profilers.

- Main Features that describe a GPU:
- Num of CUDA Cores & Memory Size
- Peak Computational Performance & Memory Bandwidth

- Salient point, GPU is used for constant predictable workloads with simple control flow.
- CUDA allows the execution of Both GPU + CPU in single code execution flow.
- CUDA can be accessed using Cuda Driver API, Cuda Runtime API. There is no performance difference, but 
they are exclusive. Using Runtime API is easier. 
 
- Ideas/Concepts Necessary for Parallel Programming:
    - Locality : Reuse of data to reduce memory latency 
        - Spatial Locality: Use of elements that is close storage location.
        - Temporal Locality: use/reuse of data elements within relatively small time durations.
    - Hirarchies of Execution exposed by Cuda:
        - Thread Hierarchy 
            - Kernel takes a serial code, and launches several threads(exec units) that perform same 
            operation inside the kernel.
            - Conceptually removing the loops in C / C++ code will provide the CUDA C code.
        - Memory Hierarchy
            - Shared memory can help in controlling the locality of your code. 
        - Barrier synchronisation: Getting the execution of the threads completed
- Programming Model: Bridge between Appln and underlying hardware.
    - Communication Boundary, User/System Boundary, Hw/Sw boundary
- Objective of Programming Model / Heirarchy:
    - Organize Threads on GPU 
    - Access the Memory on GPU
    - Domain Level(Prob soln) -> Logic Level(programming) -> Hardware Level(mapping of threads to cores)
- Use of variable names to distinguish between the host_memory and device_memory by using the h_ and d_ prefixes 
- Unified Memory bridges the Host with Device and allows to access their memories using "single pointer".
- Since the CUDA program is primarily asynchronous, the serial code on host can be complemented by the device parallel code

- Managing Memory:
    - Allocate Memory : cudaMalloc
    - Release Memory : cudaFree
    - Transfer Memory bw host n device : cudaMemcpy 
    - Global memory is analogous to CPU system memory, 
    while shared memory is similar to the CPU cache. However, GPU shared memory 
    can be directly controlled from a CUDA C kernel.
- Managing Thread:
    - GigaThread engine is a global scheduler that distributes thread blocks to the SM 
    warp schedulers.
    - All threads launched from single kernel is called a grid.
    Grid is also made of many memory blocks.
    - All threads in grid are same same global mem space.
    - Threads inside a block can cooperate using 
        - Block Local Synchronisation
        - Block Local Shared Memory
    - Threads from different blocks cannot coordinate 
    - The threads are id using threadIdx @ block level and blockIdx at grid level
        - Based on the blockIdx and threadIdx that is assigned during the 
        runtime, portion of data can be assigned to each thread.
        - Dimension of block and grid are specified using blockDim and gridDim
        - dim3 type is manually defined on host side, while its corresponding uint3
        is automatically created on the device side.
- Kernel Calling:
    - <<<grid, block>>> is used for informing how many threads the kernel should launch, 
    and what is the layout of those threads using the grid.
    - Take a 32 element vector, which can be allocated threads in <<<4, 8>>>, <<<32, 1>>>, 
    <<<1, 32>>> or even <<<8, 4>>>, all of these are valid.
        - think about the nElem + blkSize - 1 / blkSize formula, 32 + 4 - 1 / 4 will be 
        35/4 that resolves to 8, which is another way of deciding the num of blocks.
    - Kernel can be forced to execute in sequential mode using <<<1, 1>>>, and inside 
    the for loop can be executed.
    - After the kernel is called, the control is immediately returned to the C code, 
    and its other sequential activities can be done.  
    - The following restrictions apply for all kernels:
        ➤ Access to device memory only
        ➤ Must have void return type
        ➤ No support for a variable number of arguments
        ➤ No support for static variables
        ➤ No support for function pointers
        ➤ Exhibit an asynchronous behavior 
    - Calculate Row-Major index of the array by using the below formula
        rowIdx = blockIdx.x * blockDim.x + threadIdx.x
        check if rowIdx is within the array length using (rowIdx < N)
    -> For Fermi devices, the maximum number of threads per block is 1,024, and the 
        maximum grid dimension for each x, y, and z dimension is 65,535.
    - Compute to Communication Ratio is either > 1 or < 1. If > 1, then it is computing 
    for longer time, than transferring so overlap compute. If < 1, then transferring is 
    taking more time, so reduce the transfering 
- Optimizing parallel threads:
    - Three Indices to manage:
        - Thread & Block Idx, Coordinates of given data in matrix, offset in Linear Global memory
    - How to get the global Offset:
        - Find row and column major idx (ix and iy)
        - used idx = iy * nx + ix where nx is the x dim of the matrix/data used
    - Just increasing the blocks in kernel call is not going to increase performance, 
    there is a tradeoff.

- Managing devices:
    - Using CUDA Device APIs 
    - Using Nvidia SMI tool:
        - nvidia-smi -q -i 0 -d MEMORY | tail -n 5
        - nvidia-smi -q -i 0 -d UTILIZATION | tail -n 4