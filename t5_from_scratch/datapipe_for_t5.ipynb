{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchdata import datapipes as dp\n",
    "from torchtext.datasets import (\n",
    "    IMDB,\n",
    "    DBpedia,\n",
    "    CC100,\n",
    "    PennTreebank,\n",
    "    AG_NEWS,\n",
    "    YahooAnswers,\n",
    "    SQuAD2,\n",
    "    SST2\n",
    ")\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import tarfile\n",
    "from functools import partial\n",
    "from torchtext.models import (\n",
    "    T5Transform,\n",
    "    FLAN_T5_XL_GENERATION,\n",
    "    T5_BASE_GENERATION,\n",
    "    RobertaClassificationHead\n",
    ")\n",
    "from transformers import T5Tokenizer\n",
    "from torchtext.prototype.generate import GenerationUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from t5_pytorch import T5\n",
    "# importing the T5 model that has been built by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-base\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":8,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":50,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5(\n",
    "    dim=768,\n",
    "    enc_num_tokens=model_params['MAX_SOURCE_TEXT_LENGTH'],\n",
    "    enc_depth=6,\n",
    "    enc_heads=12,\n",
    "    enc_dim_head=64,\n",
    "    enc_mlp_mult=4,\n",
    "    dec_num_tokens=model_params['MAX_TARGET_TEXT_LENGTH'],\n",
    "    dec_depth=6,\n",
    "    dec_heads=12,\n",
    "    dec_dim_head=64,\n",
    "    dec_mlp_mult=4,\n",
    "    dropout=0.,\n",
    "    tie_token_emb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# calling the tokeniser from_pretrained directly works.\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='t5-base',\n",
    "    model_max_length=model_params['MAX_SOURCE_TEXT_LENGTH']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = \"D:\\\\gitFolders\\\\pytorch_hardway\\\\data\\\\news_summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    source = tokenizer.batch_encode_plus([sentence[1]],\n",
    "                                         max_length=model_params['MAX_SOURCE_TEXT_LENGTH'],\n",
    "                                         pad_to_max_length=True,\n",
    "                                         truncation=True,\n",
    "                                         padding=\"max_length\",\n",
    "                                         return_tensors='pt')\n",
    "    target = tokenizer.batch_encode_plus([sentence[0]],\n",
    "                                         max_length=model_params['MAX_TARGET_TEXT_LENGTH'],\n",
    "                                         pad_to_max_length=True,\n",
    "                                         truncation=True,\n",
    "                                         padding=\"max_length\",\n",
    "                                         return_tensors='pt')\n",
    "    source_ids = source['input_ids'].squeeze()\n",
    "    target_ids = target['input_ids'].squeeze()\n",
    "    source_mask = source['attention_mask'].squeeze()\n",
    "    target_mask = target['attention_mask'].squeeze()\n",
    "    return source_ids, source_mask, target_ids, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata import datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# when the file contains lot of rows, the pipeline takes more time to process, even to load the text and \n",
    "# provide text data\n",
    "\n",
    "news_file = dp.iter.FileLister(file_loc)\n",
    "news_open = dp.iter.FileOpener(news_file, mode='r', encoding='utf-8')\n",
    "news_parser = news_open.parse_csv(delimiter=',')\n",
    "news_embed = news_parser.map(embed_sentence)\n",
    "news_batch = news_embed.batch(8)\n",
    "news_loader = DataLoader(news_batch, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_columns = news_batch.rows2columnar(['source_ids', 'source_mask', 'target_ids', 'target_mask'])\n",
    "columns_loader = DataLoader(news_columns, batch_size=None)\n",
    "test_batch = next(iter(columns_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = test_batch['source_ids'][1]\n",
    "source_mask = test_batch['source_mask'][1]\n",
    "target = test_batch['target_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "print(source.shape)\n",
    "print(source_mask.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\gitFolders\\pytorch_hardway\\t5_from_scratch\\datapipe_for_t5.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/gitFolders/pytorch_hardway/t5_from_scratch/datapipe_for_t5.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m model(source, target, source_mask)\n",
      "File \u001b[1;32md:\\python_installs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\python_installs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\gitFolders\\pytorch_hardway\\t5_from_scratch\\t5_pytorch.py:414\u001b[0m, in \u001b[0;36mT5.forward\u001b[1;34m(self, src, tgt, mask, context_mask)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m             context_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 414\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(src)\n\u001b[0;32m    415\u001b[0m     \u001b[39m#x = x + self.pos_emb(torch.arange(x.shape[1], device = x.device))\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, mask\u001b[39m=\u001b[39mmask)\n",
      "File \u001b[1;32md:\\python_installs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\python_installs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\python_installs\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32md:\\python_installs\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "output = model(source, target, source_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.randint(0, 512, (1, 1024))\n",
    "src_mask = torch.ones_like(src).bool()\n",
    "tgt = torch.randint(0, 512, (1, 1024))\n",
    "\n",
    "output = model(src, tgt, mask=src_mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptree_train, ptree_valid, ptree_test = PennTreebank(split=('train', 'valid', 'test'))\n",
    "print(list(ptree_train)[0])\n",
    "print(list(ptree_test)[0])\n",
    "print(list(ptree_valid)[0])\n",
    "# data is all single sentences, useful for language modelling only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_train, sst2_dev, sst2_valid = SST2(split=('train','dev','test'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(sst2_dev)[9])\n",
    "print(list(sst2_train)[9])\n",
    "print(list(sst2_valid)[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sq2, test_sq2 = SQuAD2(split=('train', 'dev'))\n",
    "task = 'summarize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_sq2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prefix(task, x):\n",
    "    \"\"\"The function removes 2 columns and returns a processed tuple\"\"\" \n",
    "    return f\"{task}: \" + x[0], x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sq2 = train_sq2.map(partial(apply_prefix, task))  # A partial is created with apply_prefix, and for \n",
    "# the next variable, data is taken from pipe and applied\n",
    "# partial(apply_prefix, task) return a function to which data_point 'x', a tuple is applied\n",
    "test_sq2 = test_sq2.map(partial(apply_prefix, task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(train_sq2)[2]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sq2_batch = train_sq2.batch(8)\n",
    "test_sq2_batch = test_sq2.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = list(train_sq2_batch)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sq2_rws = test_sq2_batch.rows2columnar([\"explanation\", \"question\"])\n",
    "train_sq2_rws = train_sq2_batch.rows2columnar([\"explanation\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(train_sq2_rws)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sq2_dataloader = DataLoader(test_sq2_rws, shuffle=True, batch_size=None)\n",
    "train_sq2_dataloader = DataLoader(train_sq2_rws, shuffle=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sq2_iter = iter(train_sq2_dataloader)\n",
    "next(train_sq2_iter)  # Converts into the iterator that can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = next(train_sq2_iter)\n",
    "text = text_batch['explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data is ready for feeding, we can get the model\n",
    "padding_idx = 0\n",
    "eos_idx = 1\n",
    "max_seq_len = 512\n",
    "t5_sp_model_path = \"https://download.pytorch.org/models/text/t5_tokenizer_base.model\"\n",
    "\n",
    "transform = T5Transform(\n",
    "    sp_model_path=t5_sp_model_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    eos_idx=eos_idx,\n",
    "    padding_idx=padding_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(\"this is a test sentence\")  # tensor([  48,   19,    3,    9,  794, 7142,    1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base = T5_BASE_GENERATION\n",
    "transform = t5_base.transform()\n",
    "model = t5_base.get_model()  # model has to be in the .cache/torch/hub/checkpoints/ \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = transform(text)\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.prototype.generate import GenerationUtils\n",
    "\n",
    "sequence_generator = GenerationUtils(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in text:\n",
    "    print(s)\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beam_size = 1\n",
    "model_output = sequence_generator.generate(model_input,\n",
    "                                           eos_idx=eos_idx,\n",
    "                                           num_beams=beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = transform.decode(model_output.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in output_text:\n",
    "    print(s)\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imdb, test_imdb = IMDB(split=('train','test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_imdb)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_class = {\n",
    "    1:\"positive\",\n",
    "    2:\"negative\"\n",
    "}\n",
    "\n",
    "def label_class(point):\n",
    "    return 'sst2 sentence ' + point[1], lab_class[point[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_class_train = train_imdb.map(label_class)\n",
    "imdb_class_test = test_imdb.map(label_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(imdb_class_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_batch_train = imdb_class_train.batch(8)\n",
    "imdb_batch_test = imdb_class_test.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_rws = imdb_batch_train.rows2columnar(['statement', 'class'])\n",
    "imdb_test_rws = imdb_batch_test.rows2columnar(['statement', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_loader_train = DataLoader(imdb_train_rws, batch_size=None,)\n",
    "imdb_loader_test = DataLoader(imdb_train_rws, batch_size=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_text_data = next(iter(imdb_loader_train))\n",
    "classify_text = imdb_text_data['statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = imdb_text_data['statement']\n",
    "model_input = transform(text)\n",
    "model_input = sequence_generator.generate(model_input, eos_idx=eos_idx, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = transform.decode(model_output.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(f\"Example {i + 1}: \\n\")\n",
    "    print(f\"Input_text: {text[i]}\\n\")\n",
    "    print(f\"Output_text: {output_text[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the T5 base encoder model\n",
    "from torchtext.models import T5_BASE_ENCODER\n",
    "t5_encoder_base = T5_BASE_ENCODER\n",
    "t5_encoder_transform = t5_encoder_base.transform()\n",
    "input_seq = ['Hello there', 'Yo where is attention']\n",
    "model_input = t5_encoder_transform(input_seq)\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_b_enc_model = t5_encoder_base.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_b_enc_output = t5_b_enc_model(model_input)\n",
    "t5_b_enc_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.models import T5_BASE\n",
    "t5_base = T5_BASE\n",
    "t5_transform = t5_base.transform()\n",
    "in_seq = [\"Hello Seq\", \"Attention rocks\"]\n",
    "mod_input = t5_transform(in_seq)\n",
    "mod_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sst_transform(point):\n",
    "    return t5_transform(point[0]), point[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dp = sst2_train.map(apply_sst_transform)\n",
    "train_dp = train_dp.batch(batch_size)\n",
    "train_dp = train_dp.rows2columnar(['token_ids', 'target'])\n",
    "train_dl = DataLoader(train_dp, batch_size=None)\n",
    "\n",
    "dev_dp = sst2_dev.map(apply_sst_transform)\n",
    "dev_dp = dev_dp.batch(batch_size)\n",
    "dev_dp = dev_dp.rows2columnar(['token_ids', 'target'])\n",
    "dev_dl = DataLoader(dev_dp, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = next(iter(train_dl))\n",
    "test_text['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_dim = 768\n",
    "\n",
    "classifier_head = RobertaClassificationHead(num_classes=num_classes,\n",
    "                                            input_dim=input_dim)\n",
    "t5_model = t5_base.get_model()  # Unable to load the special classification on t5 models\n",
    "t5_model = t5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model(test_text['token_ids'][0])  # This wil throw error, as we need to provide a masked data for T5 model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "learn_rate = 1e-5\n",
    "optimiser = AdamW(t5_model.parameters(), lr=learn_rate)\n",
    "criteria = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input, target):\n",
    "    output = t5_model(input)  # get prediction \n",
    "    loss = criteria(output, target)  # get loss, !! wont this error out due to device mismatch? Nope, its taken care\n",
    "    optimiser.zero_grad()  # optimiser zeroing gradient\n",
    "    loss.backward()  # back prop\n",
    "    optimiser.step()  # updating model params\n",
    "\n",
    "\n",
    "def eval_step(input, target):\n",
    "    output = t5_model(input)  # getting pred\n",
    "    loss = criteria(output, target).item()  # getting loss\n",
    "    return float(loss), (output.argmax(1) == target).type(torch.float).sum().item()\n",
    "    # return the loss, along with the predicted output\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    t5_model.eval() # push model to eval mode\n",
    "    tot_loss = 0\n",
    "    correct_pred = 0\n",
    "    tot_pred = 0\n",
    "    counter = 0\n",
    "    # declare supporting variables\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dl:\n",
    "            input = F.to_tensor(batch[\"token_ids\"], padding_value=1).to(device)\n",
    "            target = torch.tensor(batch['target']).to(device)\n",
    "            loss, preds = eval_step(input, target)\n",
    "            total_loss += loss\n",
    "            correct_pred += preds\n",
    "            tot_pred += len(target)\n",
    "            counter += 1\n",
    "\n",
    "    return total_loss / counter, correct_pred / tot_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        input = batch['token_ids']\n",
    "        target = F.to_tensor(batch['target']).to(device)\n",
    "        train_step(input, target)\n",
    "\n",
    "    loss, acc = evaluate()\n",
    "    print(f\"Epoch = {e}, loss = {loss}, accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.models import T5Conf, T5Bundle\n",
    "\n",
    "#T5Conf is a Dataclass\n",
    "\n",
    "encoder_conf = T5Conf(encoder_only=True)\n",
    "model_checkpoint=\"C:\\\\Users\\\\kamal\\\\.cache\\\\torch\\\\hub\\\\checkpoint\\\\t5.base.encoder.v2.pt\"\n",
    "model = T5Bundle.build_model(config=encoder_conf, checkpoint=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
