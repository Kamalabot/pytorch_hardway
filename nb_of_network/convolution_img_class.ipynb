{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the example of a tensor with a shape of (1, 5, 10):\n",
    "\n",
    "Batch Size: 1 (processing one sequence at a time).\n",
    "Channels: 5 (representing information from 5 different features or sensors).\n",
    "Sequence Length: 10 (the length of each sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 10])\n",
      "torch.Size([1, 3, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3189e-01,  1.3326e+00,  2.9513e-01, -1.2541e-01,  4.8359e-01,\n",
       "           1.9740e-01,  7.8930e-01,  3.7541e-01],\n",
       "         [-3.4906e-01, -3.4798e-01, -5.7723e-02,  1.0021e+00, -5.6749e-01,\n",
       "          -3.6059e-01,  1.0436e-02, -6.7176e-01],\n",
       "         [ 9.7128e-02, -1.1572e+00,  5.4874e-01,  2.3012e-01,  3.0749e-02,\n",
       "          -8.1957e-01, -9.3595e-02,  1.3155e-03]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv0 = nn.Conv1d(in_channels=5,out_channels=3,kernel_size=3,stride=1)\n",
    "input0 = torch.randn(1, 5, 10)\n",
    "print(input0.shape)\n",
    "out = conv0(input0)\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(2, 1, kernel_size=(2,), stride=(1,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=2, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1065,  0.0325,  0.3835, -0.4821, -0.5456, -0.6344, -1.2692,\n",
       "           0.5657,  1.6293,  0.5169],\n",
       "         [-0.1785, -0.5415,  1.6034,  0.1962, -0.6446, -0.0886,  1.8281,\n",
       "          -0.3059,  0.7381, -0.1009]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 2, 10)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7086, -0.1653, -0.2917, -0.3048, -0.2009,  0.5983,  0.1331,\n",
       "          -0.8856, -1.2601]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = conv(input)\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.4575, -0.2033],\n",
       "         [ 0.1255,  0.2920]]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy Dataset for Binary Classification\n",
    "input_size_cnn1d = 100  # Adjust the input size based on your data\n",
    "num_classes_cnn1d = 1   # Binary classification\n",
    "\n",
    "cnn1d_data = torch.randn(100, 1, input_size_cnn1d)  # Batch size of 100\n",
    "cnn1d_labels = torch.randint(0, 2, (100,)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mechanically, the rows are considered as channels, and columns are part of the channel, like a signal\n",
    "\n",
    "There can be many batches of these channels and signals. The dimension of the signals are important.\n",
    "\n",
    "In examples below, when 2d data is pushed into 1d convolution, Runtime error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1160, -0.4742, -0.5641, -1.1295, -0.4921,  1.2707,  1.6504,  3.0685,\n",
       "          0.2358,  0.9147,  0.2113, -1.0332, -1.0887,  1.2971,  1.1889,  0.7151,\n",
       "          1.1036, -0.8386, -0.3371, -0.3723,  0.0834, -1.2234,  1.0445,  0.4863,\n",
       "         -1.0724,  0.3048,  0.5339, -0.7957,  0.8397, -1.7197, -0.6979,  0.5634,\n",
       "         -1.4362, -0.6159,  0.8184,  0.0582,  0.7082,  0.1869, -0.5493,  0.7642,\n",
       "          0.8636, -1.8948, -1.2380, -0.6086,  2.0619,  1.5022, -0.8410, -2.5913,\n",
       "         -0.1620, -0.3567, -0.1441,  0.9850, -1.4515, -0.6341,  0.6756, -1.2304,\n",
       "         -0.0053,  0.6021,  0.7283,  1.1197, -0.7650,  2.0131, -0.4366,  0.5713,\n",
       "         -0.5437, -0.2138, -0.7753,  0.5506,  0.6962, -0.6483,  1.7465,  0.0736,\n",
       "         -0.3780, -0.0973, -0.4908, -0.2331,  1.3002, -1.3981,  0.6399,  0.7240,\n",
       "          0.9336, -0.1400,  0.1766, -0.7851, -0.6539, -1.1359,  1.5926, -0.2918,\n",
       "         -0.3544,  1.3547,  1.1667,  1.4869,  0.6319,  0.0561,  1.3375,  0.7295,\n",
       "         -0.7983,  0.7254,  1.1199, -0.1516]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1d_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d = nn.Conv1d(in_channels=1, out_channels=5, kernel_size=3)\n",
    "conved_1d = conv1d(cnn1d_data)\n",
    "conved_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(in_channels=2, out_channels=10, kernel_size=2, stride=1)\n",
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1382, -0.0935],\n",
       "          [ 0.4611,  0.1107]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0292, -0.1385],\n",
       "          [ 0.2915, -0.0875]]]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_0_2d = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(2, 2))\n",
    "conv_0_2d.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.4646],\n",
       "         [-0.0630]]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv02 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=1, stride=1)\n",
    "conv02.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0165],\n",
       "          [ 0.2303]],\n",
       "\n",
       "         [[ 0.0955],\n",
       "          [-0.0068]]]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d2k = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=(2, 1), stride=1)\n",
    "conv1d2k.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 8, 48, 98])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_3d = torch.randn(20, 16, 10, 50, 100)\n",
    "conv3d0 = nn.Conv3d(in_channels=16,out_channels=8, kernel_size=3)\n",
    "output_3d = conv3d0(input_3d)\n",
    "output_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_0_1k = nn.Conv2d(in_channels=16, out_channels=2, kernel_size=(1, 2))\n",
    "# conv_0_1k.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending 3d input into 2d conv\n",
    "# Runtime Error: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [20, 16, 10, 50, 100]\n",
    "in3d_out2d = conv_0_1k(input_3d)\n",
    "in3d_out2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2d_data = torch.randn([5, 1, 3, 3])\n",
    "conv_2d_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_2d = conv_0_2d(conv_2d_data)\n",
    "convolved_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to understand conv1d, 2d and 3d intutively\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLQ-UNteTsc3CJ1DI974uXFkHXt8wQhXBv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional 1D network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple 1D Convolutional Neural Network for sequence classification\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * (input_size // 2), num_classes)  # observer where the input_size is applied\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CNN1D model\n",
    "input_size_cnn1d = 20\n",
    "num_classes_cnn1d = 2\n",
    "\n",
    "model_cnn1d = CNN1D(input_size_cnn1d, num_classes_cnn1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model = torch.randn(10, 1, 10)\n",
    "temp1d = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=2)\n",
    "out = temp1d(data_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OutputSequenceLength = (input_sequence_length + 2 * padding - kernel_size) / stride\n",
    "\n",
    "OutputSequenceLength = OutputSequenceLength + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_seq(in_length, padding, kernel_size, stride):\n",
    "    temp = in_length + (2 * padding) - kernel_size\n",
    "    return (temp / stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_out_seq(10, 1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_out_seq(10, 2, 3, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
