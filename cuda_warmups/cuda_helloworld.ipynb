{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BtS9drh9hVKg"
      },
      "outputs": [],
      "source": [
        "!pip install torch Ninja > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zbs79VV9lqmd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8NY4P_oxheDL"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "std::string hello_world() {\n",
        "  return \"hello world !\";\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVDb2mrXih6b"
      },
      "source": [
        " Errors:\n",
        "\n",
        " - the build_directory has to exist is readable path\n",
        "    - Created a directory specifically\n",
        "    \n",
        " - Ninja is required to load C++ extensions\n",
        "    - Resolved using pip install Ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRcxxVeVhewg",
        "outputId": "1d6e8157-137e-40e0-87db-03c322a1d6c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Emitting ninja build file /home/aicoder/tmp/build.ninja...\n",
            "Building extension module hello_mod...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=hello_mod -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/TH -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/aicoder/tmp/main.cpp -o main.o \n",
            "[2/2] c++ main.o -shared -L/home/aicoder/.local/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o hello_mod.so\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading extension module hello_mod...\n"
          ]
        }
      ],
      "source": [
        "hello_module = load_inline(\n",
        "    name='hello_mod',\n",
        "    cpp_sources=[cpp_source],\n",
        "    functions=['hello_world'],\n",
        "    verbose=True,\n",
        "    build_directory='/home/aicoder/tmp'  # this directory has to exist\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W3s4VUFkhe0q",
        "outputId": "92246adc-7b0b-48eb-e192-55a0a030cd9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello world !'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hello_module.hello_world()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URC5QCRdjgXF"
      },
      "source": [
        "Building a Square Matrix Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_yj_3TvkjH6w"
      },
      "outputs": [],
      "source": [
        "cuda_cpp_kernel = \"\"\"\n",
        "/*Here the kernel is defined where the work is being done*/\n",
        "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height){\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < height && col < width){\n",
        "      int idx = row * width + col;\n",
        "      result[idx] = matrix[idx] * matrix[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "/*Here a function is defined, that seems to return torch::Tensor obj*/\n",
        "torch::Tensor square_matrix(torch::Tensor matrix){\n",
        "  const auto height = matrix.size(0);\n",
        "  const auto width = matrix.size(1);\n",
        "\n",
        "  auto result = torch::empty_like(matrix);\n",
        "\n",
        "  /*wht is dim3: https://stackoverflow.com/questions/31141541/cuda-block-grid-dimensions-when-to-use-dim3 */\n",
        "\n",
        "  dim3 threads_per_block(16, 16);\n",
        "  dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
        "                        (height + threads_per_block.y - 1) / threads_per_block.y);\n",
        "  /*The kernel is being called below has been defined above */\n",
        "  square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
        "    matrix.data_ptr<float>(), result.data_ptr<float>(), width, height\n",
        "  );\n",
        "\n",
        "  return result;\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RSQMdvq9he4x"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"torch::Tensor square_matrix(torch::Tensor matrix);\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxOyoXY4qcrS"
      },
      "source": [
        "Error: cannot open shared object file when using load_inline()\n",
        "\n",
        "Your library is a dynamic library. You need to tell the operating system where it can locate it at runtime.\n",
        "\n",
        "To do so, we will need to do those easy steps:\n",
        "\n",
        "Find where the library is placed if you don't know it.\n",
        "\n",
        "sudo find / -name the_name_of_the_file.so\n",
        "\n",
        "Check for the existence of the dynamic library path environment variable(LD_LIBRARY_PATH)\n",
        "\n",
        "echo $LD_LIBRARY_PATH\n",
        "\n",
        "If there is nothing to be displayed, add a default path value (or not if you wish to)\n",
        "\n",
        "LD_LIBRARY_PATH=/usr/local/lib\n",
        "\n",
        "We add the desired path, export it and try the application.\n",
        "\n",
        "Note that the path should be the directory where the path.so.something is. So if path.so.something is in /my_library/path.so.something, it should be:\n",
        "\n",
        "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/my_library/\n",
        "\n",
        "- Run sudo ldconfig\n",
        "\n",
        "ldconfig creates the necessary links and cache to the most recent shared libraries found in the directories specified on the command line, in the file /etc/ld.so.conf, and in the trusted directories (/lib and /usr/lib).\n",
        "\n",
        "- Running sudo ldconfig lead to following\n",
        "\n",
        " /usr/local/lib/libtbbbind.so.3 is not a symbolic link on many other files\n",
        "\n",
        "-- Solution:\n",
        "\n",
        "  Simply change the extension name\n",
        "\n",
        "Error: CUDA_HOME environment variable is not set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZifhuwLq07g",
        "outputId": "582ecc0d-9cfd-4d8c-8f7b-da50ca2f6726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-12.4/lib64\n"
          ]
        }
      ],
      "source": [
        "!echo $LD_LIBRARY_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gZWV1gONhe9C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The input conditions for extension module sqr_mat_ext1 have changed. Bumping to version 1 and re-building as sqr_mat_ext1_v1...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /home/aicoder/sqr_mat_ext/build.ninja...\n",
            "Building extension module sqr_mat_ext1_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=sqr_mat_ext1_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/TH -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/aicoder/sqr_mat_ext/main.cpp -o main.o \n",
            "[2/3] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=sqr_mat_ext1_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/TH -isystem /home/aicoder/.local/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O2 -std=c++17 -c /home/aicoder/sqr_mat_ext/cuda.cu -o cuda.cuda.o \n",
            "[3/3] c++ main.o cuda.cuda.o -shared -L/home/aicoder/.local/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.4/lib64 -lcudart -o sqr_mat_ext1_v1.so\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading extension module sqr_mat_ext1_v1...\n"
          ]
        }
      ],
      "source": [
        "sqr_mat_ext = load_inline(\n",
        "    name='sqr_mat_ext1',\n",
        "    cpp_sources=[cpp_source],\n",
        "    cuda_sources=cuda_cpp_kernel,\n",
        "    functions=['square_matrix'],\n",
        "    with_cuda=True,\n",
        "    extra_cuda_cflags=[\"-O2\"],\n",
        "    verbose=True,\n",
        "    build_directory='/home/aicoder/sqr_mat_ext',\n",
        "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hello cuda\n",
        "\n",
        "cuda_code = \"\"\"\n",
        "__global__ void helloCUDA(float f)\n",
        "{\n",
        "    printf(\"Hello thread %d\", f=%f\\n, threadIdx.x, f)\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    helloCUDA<<<1, 5>>>(1.2345f);\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cpp_source = \"int main();\"\n",
        "hellomod = load_inline(\n",
        "    name='hellomod',\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_code,\n",
        "    functions=['main'],\n",
        "    with_cuda=True,\n",
        "    verbose=True,\n",
        "    extra_cflags=['-O2'],\n",
        "    build_directory='content'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "tens1 = torch.randint(0, 10, size=(1, 10))\n",
        "tens1 = torch.rand(size=(1,10))\n",
        "tens1.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         7.7052e+31, 1.9447e+31, 2.1715e-18, 2.3081e-12]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sqr_mat_ext.square_matrix(tens1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "I7Pj24eiolEk"
      },
      "outputs": [],
      "source": [
        "# Another cuda kernel\n",
        "\n",
        "cuda_kernel = \"\"\"\n",
        "extern \"C\" __global__\n",
        "void square_kernel(const float* __restrict__ input, float* __restrict__ output, int size){\n",
        "  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (index < size) {\n",
        "    output[index] = input[index] * input[index];\n",
        "  }\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lm2GP37WvsS4",
        "outputId": "ed10ae6c-5083-4bd5-9bab-e9e1ffa58577"
      },
      "outputs": [],
      "source": [
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "module = load_inline(\n",
        "    name='square',\n",
        "    cpp_sources='',\n",
        "    cuda_sources=cuda_kernel,\n",
        "    functions=['square_kernel'],\n",
        "    verbose=True\n",
        ")\n",
        "# error: ‘square_kernel’ was not declared in this scope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def square(input):\n",
        "    output = torch.empty_like(input)\n",
        "    threads_per_block = 1024\n",
        "    blocks_per_grid = (input.numel() + (threads_per_block - 1)) // threads_per_block\n",
        "    module.square_kernel(blocks_per_grid, threads_per_block, input, output, input.numel())\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor = torch.randn(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor.to(device)\n",
        "# Throwing illegal memory access when moving to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_pytorch_function(func, input):\n",
        "    # CUDA IS ASYNC so can't use python time module\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        func(input)\n",
        "\n",
        "    start.record()\n",
        "    func(input)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end)\n",
        "\n",
        "def square_2(a):\n",
        "    return a * a\n",
        "\n",
        "def square_3(a):\n",
        "    return a ** 2\n",
        "b = torch.randn(10000, 10000).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.7377279996871948"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_pytorch_function(torch.square, b)\n",
        "time_pytorch_function(square_2, b)\n",
        "time_pytorch_function(square_3, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "STAGE:2024-05-11 11:32:45 17512:17512 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
            "STAGE:2024-05-11 11:32:45 17512:17512 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
            "STAGE:2024-05-11 11:32:45 17512:17512 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd.profiler import profile\n",
        "\n",
        "with profile(use_cuda=True) as prof:\n",
        "    torch.square(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             aten::square         6.77%     135.000us        17.86%     356.000us     356.000us     135.000us         6.65%       2.030ms       2.030ms             1  \n",
            "                aten::pow         9.93%     198.000us        10.94%     218.000us     218.000us       1.887ms        92.96%       1.895ms       1.895ms             1  \n",
            "        aten::result_type         0.05%       1.000us         0.05%       1.000us       1.000us       4.000us         0.20%       4.000us       4.000us             1  \n",
            "                 aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.20%       4.000us       4.000us             1  \n",
            "          cudaEventRecord         0.75%      15.000us         0.75%      15.000us       1.875us       0.000us         0.00%       0.000us       0.000us             8  \n",
            "         cudaLaunchKernel         0.75%      15.000us         0.75%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        81.74%       1.629ms        81.74%       1.629ms       1.629ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.993ms\n",
            "Self CUDA time total: 2.030ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::mul         0.85%      79.000us         1.23%     114.000us     114.000us       1.780ms       100.00%       1.780ms       1.780ms             1  \n",
            "          cudaEventRecord        81.47%       7.553ms        81.47%       7.553ms       3.776ms       0.000us         0.00%       0.000us       0.000us             2  \n",
            "         cudaLaunchKernel         0.38%      35.000us         0.38%      35.000us      35.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        17.30%       1.604ms        17.30%       1.604ms       1.604ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 9.271ms\n",
            "Self CUDA time total: 1.780ms\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "STAGE:2024-05-11 11:34:11 17512:17512 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
            "STAGE:2024-05-11 11:34:11 17512:17512 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
            "STAGE:2024-05-11 11:34:11 17512:17512 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
          ]
        }
      ],
      "source": [
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    square_2(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::pow        28.21%     110.000us        42.05%     164.000us     164.000us       1.825ms        99.89%       1.827ms       1.827ms             1  \n",
            "        aten::result_type         1.03%       4.000us         1.03%       4.000us       4.000us       1.000us         0.05%       1.000us       1.000us             1  \n",
            "                 aten::to         0.26%       1.000us         0.26%       1.000us       1.000us       1.000us         0.05%       1.000us       1.000us             1  \n",
            "          cudaEventRecord        10.51%      41.000us        10.51%      41.000us       6.833us       0.000us         0.00%       0.000us       0.000us             6  \n",
            "         cudaLaunchKernel         9.23%      36.000us         9.23%      36.000us      36.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        50.77%     198.000us        50.77%     198.000us     198.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 390.000us\n",
            "Self CUDA time total: 1.827ms\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "STAGE:2024-05-11 11:34:26 17512:17512 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
            "STAGE:2024-05-11 11:34:26 17512:17512 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
            "STAGE:2024-05-11 11:34:26 17512:17512 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
          ]
        }
      ],
      "source": [
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    square_3(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
